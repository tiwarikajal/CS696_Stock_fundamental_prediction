{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This notebook is to clean and scale the extracted data. Currently scraped data has many placeholders like 'cash flow not found'\n",
    "## or 'income statement not found' which needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Loop this cell to clean for multiple years\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"Data_2011_3k.csv\")\n",
    "df_rev=df[df['Revenue']!='0']\n",
    "df_rev=df_rev[df_rev['Revenue']!='Income statement not found']\n",
    "df_rev.drop('NetIncomeLoss',inplace=True,axis=1)\n",
    "df_rev.drop('Payments to acquire property',inplace=True,axis=1)\n",
    "df_rev.drop_duplicates(subset =\"Revenue\",                      keep = 'first', inplace = True) \n",
    "df_rev.to_csv(\"Clean_Rev_3k_2011.csv\",index = False, header=True)\n",
    "df_inc=df[df['NetIncomeLoss']!='0']\n",
    "df_inc=df_inc[df_inc['NetIncomeLoss']!='cash flow not found']\n",
    "df_inc.drop('Revenue',inplace=True,axis=1)\n",
    "df_inc.drop('Payments to acquire property',inplace=True,axis=1)\n",
    "df_inc.drop_duplicates(subset =\"NetIncomeLoss\", \n",
    "                     keep = 'first', inplace = True) \n",
    "df_inc.to_csv(\"Clean_NetIncomeLoss_3k_2011.csv\",index = False, header=True)\n",
    "df_pay=df[df['Payments to acquire property']!='tag not found']\n",
    "df_pay=df_pay[df_pay['Payments to acquire property']!='cash flow not found']\n",
    "df_pay.drop('Revenue',inplace=True,axis=1)\n",
    "df_pay.drop('NetIncomeLoss',inplace=True,axis=1)\n",
    "df_pay.drop_duplicates(subset =\"Payments to acquire property\", \n",
    "                     keep = 'first', inplace = True) \n",
    "df_pay.to_csv(\"Clean_Payments_3k_2011.csv\",index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Once data is cleaned, it needs to be divided by companies and by number of datapoints for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loop this cell for all fundamentals\n",
    "df1=pd.read_csv(\"Clean_NetIncomeLoss_3k_2011.csv\")\n",
    "df2=pd.read_csv(\"Clean_NetIncomeLoss_3k_2012.csv\")\n",
    "df3=pd.read_csv(\"Clean_NetIncomeLoss_3k_2013.csv\")\n",
    "df4=pd.read_csv(\"Clean_NetIncomeLoss_3k_2014.csv\")\n",
    "df5=pd.read_csv(\"Clean_NetIncomeLoss_3k_2015.csv\")\n",
    "df6=pd.read_csv(\"Clean_NetIncomeLoss_3k_2016.csv\")\n",
    "df7=pd.read_csv(\"Clean_NetIncomeLoss_3k_2017.csv\")\n",
    "df8=pd.read_csv(\"Clean_NetIncomeLoss_3k_2018.csv\")\n",
    "df9=pd.read_csv(\"Clean_NetIncomeLoss_3k_2019.csv\")\n",
    "df_f=pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9])\n",
    "list='AAXN,AJRD,ATRO,BA,CW,DCO,GD,HII,KTOS,LMT,MRCY,NOC,PKE,SPR,TDG,TXT,CHRW,FWRD,HUBG,XPO,AAL,ALK,DAL,JBLU,DORM,FOXF,LCII,SMP,THO,TSLA,WGO,ABCB,AMNB,BANC,BANR,BCBP,BDGE,BHB,BKU,BMRC,BMTC,BOH,BPOP,BSRR,BUSE,BWFG,CAC,CBU,CCBG,CCNE,CFFI,CFR,CHCO,CHMG,CIVB,CNBKA,COLB,CPF,CSFL,CTBI,CUBI,CVBF,CVCY,CVLY,DCOM,EBTC,EFSC,EGBN,ESXB,EVBN,EWBC,FBIZ,FBNC,FCBC,FCCY,FCF,FCNCA,FDBC,FFBC,FFIC,FFIN,FFWM,FGBI,FIBK,FLIC,FMBH,FMNB,FNB,FRAF,FRME,FSB,FULT,GABC,GBCI,HAFC,HBMD,HBNC,HFWA,HOMB,HOPE,HTBK,HWBK,IBCP,IBTX,INBK,INDB,ISBC,ISTR,JPM,LBAI,LCNB,LKFN,MCBC,MFNC,MFSF,MOFG,MPB,MVBF,NBHC,NBN,NKSH,NWFL,ONB,ORRF,OSBC,OVBC,PB,PBCT,PEBK,PEBO,PFBI,PFIS,PGC,PMBC,PRK,RBCAA,RF,RNST,SBBX,SFST,SHBI,SLCT,SONA,SSB,STBA,TBBK,TCBI,TCBK,TCF,TCFC,THFF,TMP,TRMK,TSC,UBFO,UBSI,UMBF,UNB,UNTY,UVSP,VBTX,VLY,WABC,WAL,WASH,WBS,WSBC,WTBA,WTFC,BREW,MGPI,SAM,STZ,ACAD,ADMS,ADVM,AGIO,ALBO,ALNY,AMGN,ARNA,ATHX,BCRX,BLUE,BMRN,CCXI,CLVS,CNCE,CYTK,DRNA,DVAX,ENTA,EPZM,ESPR,EXEL,FATE,FOLD,GALT,GERN,HALO,INSM,MGNX,MNKD,MRNA,MRTX,MTEM,MYGN,NBIX,NVAX,PBYI,PDLI,PFNX,PGNX,PRTA,PTCT,PTLA,REGN,SAGE,SGEN,SGMO,SPPI,SRNE,SRPT,SVRA,VCEL,VNDA,XLRN,AAON,AMWD,APOG,AWI,BLDR,GFF,JCI,LII,PGTI,ROCK,TREX,AMTD,BCOR,FDS,GS,LPLA,MKTX,MSCI,SCHW,SF,TROW,VALU,VRTS,WETF,WHG,APD,BCPC,FTK,HWKN,IOSP,KRO,LXU,NEU,OLN,OMN,RYAM,SXT,TG,TREC,TSE,ABM,ACCO,ADSW,BRC,BV,CIX,DLX,EBF,HCSG,KAR,KBAL,KNL,MGRC,SCS,UNF,VSEC,AAOI,ADTN,ANET,CAMP,CIEN,COMM,CSCO,DZSI,EXTR,FFIV,HLIT,INFN,INSG,JNPR,KVHI,MSI,NTCT,NTGR,PLT,AMRC,DY,GLDD,IESC,MYRG,NWPX,STRL,TPC,EXP,USCR,VMC,CACC,COF,DFS,FCFS,GDOT,LC,RM,BERY,PKG,CORE,LKQ,APEI,CHGG,CSV,HMHC,LOPE,SERV,STRA,CTL,IRDM,DUK,LNT,NEE,OTTR,PNM,XEL,AMSC,AYI,GNRC,POWL,ST,WIRE,AVT,AVX,AXE,BHE,BMI,CDW,CGNX,CNXN,COHR,CTS,DAKT,FARO,FLIR,FN,GLW,KN,MTSC,NATI,NSIT,NSSC,PAR,PLUS,PLXS,ROG,SANM,SCSC,ZBRA,DO,DRQ,GEOS,HP,NR,RES,SLCA,TDW,ATVI,GLUU,NFLX,TTWO,WWE,ZNGA,CASY,CHEF,RAD,SFM,SPTN,UNFI,VLGEA,WMK,ALCO,FARM,FLO,HSY,JBSS,LANC,POST,SAFM,SENEA,THS,ATO,CPK,NFG,NJR,ABMD,ALGN,ATEC,ATRC,ATRI,BAX,BDX,CERS,CNMD,CRY,CSII,DHR,DXCM,GMED,HOLX,IART,ICUI,ISRG,ITGR,LMAT,NVRO,OFIX,OSUR,PODD,QDEL,RMD,RTIX,SRDX,STAA,SYK,TRXC,VAR,VIVO,ABC,ACHC,ADUS,AMN,ANTM,CAH,ENZ,GEN,HQY,HUM,MOH,PDCO,PRSC,RCM,TVTY,CERN,CSLT,HMSY,HSTM,MDRX,OMCL,VEEV,BJRI,CBRL,CHUY,DENN,DIN,DPZ,EAT,EVRI,FRGI,JACK,LOCO,MAR,NATH,NDLS,PENN,RCL,RRGB,SEAS,SGMS,TAST,VAC,BSET,BZH,ETH,FLXS,GPRO,GRMN,HOFT,IBP,IRBT,KBH,LCUT,MDC,MHO,MTH,NVR,PHM,TOL,TPH,TUP,UEIC,CLX,WDFC,CSL,ROP,AEL,AFL,AGO,AIZ,ALL,AMBC,AMSF,BRO,CB,CIA,CINF,CNA,EHTH,EIG,FNHC,GBLI,GLRE,HCI,HRTG,LNC,MBI,MCY,ORI,PGR,PRA,RGA,THG,UFCS,UNM,UVE,WRB,DHX,FB,QNST,TWTR,TZOO,YELP,AMZN,GRUB,PETS,QUOT,STMP,W,AKAM,BAH,CASS,CSGS,CTSH,EVTC,EXLS,FISV,FLT,G,GTT,HCKT,IT,JKHY,LLNW,PRFT,PRGX,SRT,SYKE,AOBC,BC,CLAR,ELY,ESCA,JOUT,MAT,MPX,PII,RGR,AXDX,FLDM,ILMN,LMNX,MTD,NEO,NSTG,PACB,PKI,TECH,ALG,ALSN,B,BGG,CIR,CR,DCI,DOV,ERII,ESE,FSS,GHM,GRC,HURC,IEX,JBT,LNN,MIDD,MWA,NDSN,NNBR,NPO,PRLB,ROLL,RXN,SWK,SXI,TKR,WAB,WNC,WWD,XYL,ETM,EVC,GTN,HMTV,MCHX,MDP,MSGN,NXST,SCOR,SGA,SIRI,TTGT,CDE,CENX,CMP,GORO,HL,MTRN,ZEUS,AGNC,AI,ANH,CHMI,CIM,CMO,MFA,PMT,RWT,D,NWE,PEG,UTL,WEC,BIG,DDS,DLTR,JCP,JWN,KSS,M,AXAS,CLR,CPE,CRK,DNR,EOG,ESTE,GPOR,KOS,LNG,MRO,MTDR,MUR,NBL,OSG,PARR,QEP,SBOW,SWN,TELL,UNT,WTI,XEC,CLW,GLT,NP,UFS,VRS,EPC,HLF,LFVN,NUS,REV,USNA,AKRX,AMPH,ANIP,BDSI,LLY,NKTR,OMER,PAHC,PCRX,PRGO,PRTK,RVNC,SUPN,TBPH,ZGNX,ASGN,BBSI,CBZ,CSGP,EXPO,FC,FCN,FORR,GPX,HSII,HURN,ICFI,KFRC,KFY,NSP,RHI,TBI,TNET,VRSK,APLE,BRX,CXW,DRH,GEO,GLPI,GNL,GOOD,GTY,LAMR,RESI,RHP,SNR,SRC,UHT,FRPH,MLP,MMI,ARCB,CVTI,HTLD,JBHT,KNX,LSTR,R,RRTS,SAIA,UHAL,ULH,UNP,WERN,ADI,AEIS,AMAT,AMBA,AMD,AOSL,CCMP,CEVA,COHU,CREE,CRUS,DSPG,ENPH,FORM,INTC,IPHI,KLAC,LRCX,LSCC,MCHP,MKSI,MPWR,MRVL,MTSI,MXIM,NPTN,NVDA,OLED,POWI,RMBS,SYNA,TER,TXN,UCTT,XLNX,ACIW,ADBE,ADSK,AGYS,AMSWA,ANSS,ATEN,AZPN,BLKB,BNFT,CDNS,CRM,CSOD,CTXS,CVLT,DMRC,ECOM,EGHT,ENV,EPAY,FEYE,FICO,FIVN,FTNT,HUBS,INTU,LOGM,LPSN,MANH,MODN,MSTR,NUAN,PANW,PCTY,PEGA,PFPT,PRGS,PRO,PTC,QLYS,QTWO,RNG,RP,SNCR,SPSC,SSNC,TDC,TLRA,TNAV,TWOU,TYL,VHC,VMW,VRNS,WK,ZEN,ZIXI,AAN,AAP,ABG,AEO,AN,ASNA,AZO,BKE,BURL,CATO,CHS,CONN,EXPR,FIVE,FL,GNC,GPI,GPS,HD,HIBB,HVT,HZO,LAD,LL,LOW,MNRO,ORLY,RCII,RH,ROST,SAH,SCVL,SIG,SNBR,TCS,TJX,TLYS,TSCO,ULTA,URBN,WSM,ZUMZ,AAPL,IMMR,NTAP,WDC,CRI,CROX,CULP,DLA,HBI,RL,TPR,VNCE,VRA,CASH,CFFN,EBSB,ESNT,FDEF,FSBW,GCBC,HMST,KRNY,MTG,NMIH,NYCB,OCFC,PFS,RDN,TFSL,TRST,WAFD,WD,WNEB,WSBF,AYR,BECN,BMCH,BXC,DNOW,DXPE,FAST,GATX,HEES,LAWS,MRC,MSM,RUSHA,TRNS,URI,WLFC,ARTNA,AWK,MSEX,GOGO,SHEN,SPOK,TMUS'\n",
    "ticker_list=list.split(',')\n",
    "CompanyPartialData=[]\n",
    "for i in ticker_list:\n",
    "    df_final = pd.DataFrame(columns=['year','Company','CIK','NetIncomeLoss','BaselineOutput'])\n",
    "    filename7='.\\\\7datapoints\\\\'+ i + '.csv'\n",
    "    filename6='.\\\\6datapoints\\\\'+ i + '.csv'\n",
    "    filename8='.\\\\8datapoints\\\\'+ i + '.csv'\n",
    "    filename5='.\\\\5datapoints\\\\'+ i + '.csv'\n",
    "    filename9='.\\\\9datapoints\\\\'+ i + '.csv'\n",
    "    filename4='.\\\\4datapoints\\\\'+ i + '.csv'\n",
    "#     filename2='C:\\\\Umass spring 20\\\\696\\KT\\\\CS696\\\\cs696_scraping\\\\structured\\\\Data 11-18\\\\Company Wise IncomeLoss\\\\2datapoints\\\\'+ i + '.csv'\n",
    "#     filename1='C:\\\\Umass spring 20\\\\696\\KT\\\\CS696\\\\cs696_scraping\\\\structured\\\\Data 11-18\\\\Company Wise IncomeLoss\\\\1datapoints\\\\'+ i + '.csv'\n",
    "    for index, row in df_f.iterrows():\n",
    "        if row['Company']==i:\n",
    "            df_final=df_final.append(pd.Series([row['year'], row['Company'], row['CIK'], row['NetIncomeLoss'],'NULL'], index=df_final.columns ), ignore_index=True)\n",
    "    if len(df_final.index)==8:\n",
    "        df_final.to_csv(filename8,index = False, header=True)\n",
    "    if len(df_final.index)==7:\n",
    "        df_final.to_csv(filename7,index = False, header=True)\n",
    "    if len(df_final.index)==6:\n",
    "        df_final.to_csv(filename6,index = False, header=True)\n",
    "    if len(df_final.index)==5:\n",
    "        df_final.to_csv(filename5,index = False, header=True)\n",
    "    if len(df_final.index)==9:\n",
    "        df_final.to_csv(filename9,index = False, header=True)\n",
    "    if len(df_final.index)==4:\n",
    "        df_final.to_csv(filename4,index = False, header=True)   \n",
    "#     if len(df_final.index)==2:\n",
    "#         df_final.to_csv(filename2,index = False, header=True)  \n",
    "#     if len(df_final.index)==1:\n",
    "#         df_final.to_csv(filename1,index = False, header=True)  \n",
    "    elif len(df_final.index)<4:\n",
    "        CompanyPartialData.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cell below scale the data for each company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "##Path where companywise files are located\n",
    "path=\"C:\\\\Umass spring 20\\\\696\\\\KT\\\\CS696\\\\CS696\\\\structuredDataExtraction\\\\Data_Pre_processing\\\\data_rev\\\\\"\n",
    "os.chdir(path)\n",
    "\n",
    "for filename in os.listdir(): \n",
    "#     os.chdir(out_path)\n",
    "    df=pd.DataFrame()  \n",
    "    if filename.endswith(\".csv\"):  \n",
    "    \n",
    "        df=pd.read_csv(filename) \n",
    "        df=df.sort_values(by='year') \n",
    "        \n",
    "        if len(df.index)>2: \n",
    "          #print(filename)  \n",
    "          #print(df['embeddings1a'])\n",
    "          temp1=temp2=pd.DataFrame()\n",
    "          \n",
    "          temp1=df['Revenue'].shift(1)\n",
    "          temp2=df['Revenue'].shift(2)\n",
    "          temp3=df['Revenue'].shift(3)\n",
    "          df['t-1Revenue']=temp1\n",
    "          df['t-2Revenue']=temp2\n",
    "          df['t-3Revenue']=temp3\n",
    "          df.dropna(subset = [\"t-1Revenue\",'t-2Revenue','t-3Revenue'], inplace=True)\n",
    "          df.drop('BaselineOutput',axis=1,inplace=True)\n",
    "          for i,r in df.iterrows():\n",
    "            temp11=((r['Revenue'])-(r['t-1Revenue']))/(abs(r['Revenue'])+abs(r['t-1Revenue']))\n",
    "            temp22=((r['t-1Revenue'])-(r['t-2Revenue']))/(abs(r['t-1Revenue'])+abs(r['t-2Revenue']))\n",
    "            temp33=((r['t-2Revenue'])-(r['t-3Revenue']))/(abs(r['t-1Revenue'])+abs(r['t-2Revenue']))\n",
    "            df.loc[i,'Revenue']=temp11\n",
    "            df.loc[i,'t-1Revenue']=temp22\n",
    "            df.loc[i,'t-2Revenue']=temp33\n",
    "          df.drop('t-3Revenue',axis=1,inplace=True)\n",
    "          df=df[~df.Revenue.astype(str).str.contains(\"0.99\")]\n",
    "          df=df[~df['t-1Revenue'].astype(str).str.contains(\"0.99\")] \n",
    "          print(df.head())\n",
    "          df.to_csv(r\"C:\\\\Umass spring 20\\\\696\\\\KT\\\\CS696\\\\CS696\\\\structuredDataExtraction\\\\Data_Pre_processing\\\\data_rev\\\\window\\\\\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below cell creates one consolidated file for every fundamental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Path where scaled companywise files are located\n",
    "path=\"C:\\\\Umass spring 20\\\\696\\\\KT\\\\CS696\\\\CS696\\\\structuredDataExtraction\\\\Data_Pre_processing\\\\data_rev\\\\window\\\\\"\n",
    "os.chdir(path)\n",
    "final=pd.DataFrame()\n",
    "for filename in os.listdir(): \n",
    "  temp=pd.DataFrame()\n",
    "  if filename.endswith('.csv'):\n",
    "    \n",
    "     temp=pd.read_csv(filename)\n",
    "     final=pd.concat([final,temp])\n",
    "\n",
    "final.to_csv('Augmented_Dataset.csv',index=False,header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below cell creates one pickle file in format which will be input to model (for only structured data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=pd.read_csv('Augmented_Dataset.csv')\n",
    "final_df=pd.DataFrame(columns=['x','y'])\n",
    "\n",
    "\n",
    "for i, r in a.iterrows():\n",
    "    x=[float(r['t-1Revenue']),float(r['t-2Revenue'])]\n",
    "    final_df=final_df.append(pd.Series([x,r['Revenue']], index=final_df.columns ), ignore_index=True)\n",
    "     \n",
    "\n",
    "print(final_df)\n",
    "final_df.to_pickle('Final_dataset.pkl')\n",
    "df.y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
