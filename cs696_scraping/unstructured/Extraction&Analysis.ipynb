{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sec_edgar_downloader import Downloader\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "import csv\n",
    "import shutil\n",
    "import unicodedata\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set the location where all the downloaded filings will be saved\n",
    "downloadPath = \"./\"\n",
    "dl = Downloader(downloadPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ticker = \"AAPL\"\n",
    "# ticker = \"C\"\n",
    "# pos_dat = None\n",
    "document = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Download 10 latest 10-K filings for the given company ticker\n",
    "def downloadFilings(ticker, filing_type=\"10-K\", latest=10):\n",
    "    dl.get(filing_type, ticker, latest)\n",
    "\n",
    "def removefilings(path):\n",
    "    shutil.rmtree(path, ignore_errors=True)\n",
    "# downloadFilings(\"BRK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadFilings(ticker, \"10-Q\", latest=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "itemStartEndMapping = {\"item1\":\"item1a\",\"item1a\":\"item1b\",\"item1b\":\"item2\", \"item2\":\"item3\",\"item3\":\"item4\",\"item4\":\"item5\",\n",
    "                      \"item5\":\"item6\", \"item6\":\"item7\",\"item7\":\"item7a\",\"item7a\":\"item8\", \"item8\":\"item9\",\"item9\":\"item9a\",\n",
    "                       \"item9a\":\"item9b\",\"item9b\":\"item10\",\"item10\":\"item11\",\"item11\":\"item12\",\"item12\":\"item13\",\n",
    "                       \"item13\":\"item14\",\"item14\":\"item15\",\"item15\":\"item16\"}\n",
    "def extractHTMLSections(raw_10k):\n",
    "    document = {}\n",
    "#     global pos_dat\n",
    "    # Regex to find <DOCUMENT> tags\n",
    "    doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
    "    doc_end_pattern = re.compile(r'</DOCUMENT>')\n",
    "    \n",
    "    # Regex to find <TYPE> tag prceeding any characters, terminating at new line\n",
    "    type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
    "    \n",
    "    # Create 3 lists with the span idices for each regex\n",
    "    \n",
    "    ### There are many <Document> Tags in this text file, each as specific exhibit like 10-K, EX-10.17 etc\n",
    "    ### First filter will give us document tag start <end> and document tag end's <start> \n",
    "    ### We will use this to later grab content in between these tags\n",
    "    doc_start_is = [x.end() for x in doc_start_pattern.finditer(raw_10k)]\n",
    "    doc_end_is = [x.start() for x in doc_end_pattern.finditer(raw_10k)]\n",
    "\n",
    "    ### Type filter is interesting, it looks for <TYPE> with Not flag as new line, ie terminare there, with + sign\n",
    "    ### to look for any char afterwards until new line \\n. This will give us <TYPE> followed Section Name like '10-K'\n",
    "    ### Once we have have this, it returns String Array, below line will with find content after <TYPE> ie, '10-K' \n",
    "    ### as section names\n",
    "    doc_types = [x[len('<TYPE>'):] for x in type_pattern.findall(raw_10k)]\n",
    "\n",
    "    # Create a loop to go through each section type and save only the 10-K section in the dictionary\n",
    "    for doc_type, doc_start, doc_end in zip(doc_types, doc_start_is, doc_end_is):\n",
    "        if doc_type == '10-K' and doc_type not in document.keys():\n",
    "            document[doc_type] = raw_10k[doc_start:doc_end]\n",
    "    \n",
    "    # Write the regex to get different sections from the 10-K\n",
    "    regex = re.compile(r'(>(\\s|&#160;|&nbsp;)*item(\\s|&#160;|&nbsp;)*(1(\\s|&nbsp;|&#160;|&#160;\\(|\\(|)*a|6|1(\\s|&nbsp;|&#160;|&#160;\\(|\\(|)*b|7(\\s|&nbsp;|&#160;|&#160;\\(|\\(|)*a|7|8|2|4)\\.{0,1})', re.I)\n",
    "    \n",
    "    # Use finditer to math the regex\n",
    "    matches = regex.finditer(document['10-K'])\n",
    "    \n",
    "    # Create the dataframe\n",
    "    test_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])\n",
    "\n",
    "    test_df.columns = ['item', 'start', 'end']\n",
    "    test_df['item'] = test_df.item.str.lower()\n",
    "    \n",
    "    # Get rid of unnesesary charcters from the dataframe\n",
    "    test_df.replace('&#160;ris',' ',regex=True,inplace=True)\n",
    "    test_df.replace('&#160;unresolve',' ',regex=True,inplace=True)\n",
    "    test_df.replace('&#160;',' ',regex=True,inplace=True)\n",
    "    test_df.replace('&nbsp;',' ',regex=True,inplace=True)\n",
    "    test_df.replace(' ','',regex=True,inplace=True)\n",
    "    test_df.replace('\\.','',regex=True,inplace=True)\n",
    "    test_df.replace('>','',regex=True,inplace=True)\n",
    "    test_df.replace('\\n','',regex=True,inplace=True)\n",
    "    \n",
    "    # Aggregate the different parts of the sane section\n",
    "    pos_dat = test_df.groupby(['item']).agg({'start': utils.customsort, 'end': 'max'})\n",
    "    print(\"Sections Extracted:{}\".format(list(pos_dat.index)))\n",
    "#     print(pos_dat)\n",
    "    return list(pos_dat.index), pos_dat, document\n",
    "\n",
    "def extractTextFromSection(key,pos_dat, document,document_type = \"10-K\"):\n",
    "    if key in list(pos_dat.index):\n",
    "        if document_type not in document:\n",
    "            print(\"{} not found\".format(document_type))\n",
    "            # Get Item 1a\n",
    "        else:\n",
    "            end = \"\"\n",
    "            original = key\n",
    "            while True:\n",
    "                if original not in itemStartEndMapping.keys():\n",
    "                    break\n",
    "                elif itemStartEndMapping[original] not in list(pos_dat.index) or pos_dat[\"start\"].loc[key]>= pos_dat[\"start\"].loc[itemStartEndMapping[original]]:\n",
    "                    original = itemStartEndMapping[original]\n",
    "                else:\n",
    "                    end = itemStartEndMapping[original]\n",
    "                    break\n",
    "            if len(end) == \"0\":\n",
    "                print(\"Error Cannnot find End tag for {}\".format(key))\n",
    "                return\n",
    "            else:                \n",
    "                item_raw = document['10-K'][pos_dat['start'].loc[key]:pos_dat['end'].loc[end]]\n",
    "                item_raw_content = BeautifulSoup(item_raw, \"lxml\")\n",
    "                content = item_raw_content.get_text()\n",
    "                content = unicodedata.normalize(\"NFKD\", content)\n",
    "                content = content.replace(\"\\n\", \" \")\n",
    "                text = content.replace(\"  \", \" \")\n",
    "                content = content.lower()\n",
    "                return content\n",
    "    else:\n",
    "        print(\"{} not found\".format(key))\n",
    "        return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections Extracted:['item1a', 'item1b', 'item2', 'item4', 'item6', 'item7', 'item7a', 'item8']\n",
      "Extracted MDA text Length:60016\n",
      "Sections Extracted:['item1a', 'item1b', 'item2', 'item4', 'item6', 'item7', 'item7a', 'item8']\n",
      "Extracted MDA text Length:56851\n",
      "Sections Extracted:['item1a', 'item1b', 'item2', 'item4', 'item6', 'item7', 'item7a', 'item8']\n",
      "Extracted MDA text Length:33501\n",
      "Sections Extracted:['item1a', 'item1b', 'item2', 'item6', 'item7', 'item7a', 'item8']\n",
      "Extracted MDA text Length:65410\n",
      "Sections Extracted:['item1a', 'item1b', 'item2', 'item6', 'item7', 'item7a', 'item8']\n",
      "Extracted MDA text Length:61832\n",
      "Sections Extracted:['item1a', 'item1b', 'item2', 'item4', 'item6', 'item7', 'item7a', 'item8']\n",
      "Extracted MDA text Length:63361\n",
      "Sections Extracted:['item1a', 'item1b', 'item2', 'item4', 'item6', 'item7', 'item7a', 'item8']\n",
      "Extracted MDA text Length:65905\n",
      "Sections Extracted:['item1a', 'item1b', 'item2', 'item4', 'item6', 'item7', 'item7a', 'item8']\n",
      "Extracted MDA text Length:68342\n",
      "Sections Extracted:['item1a', 'item1b', 'item2', 'item4', 'item6', 'item7', 'item7a', 'item8']\n",
      "Extracted MDA text Length:64159\n",
      "Sections Extracted:['item1a', 'item1b', 'item2', 'item4', 'item6', 'item7', 'item7a', 'item8']\n",
      "Extracted MDA text Length:58349\n"
     ]
    }
   ],
   "source": [
    "''' SEC-Edgar module creates a folder 'sec_edgar_filings' in which it downloades the filings '''\n",
    "newPath = downloadPath + \"sec_edgar_filings\"\n",
    "companyFilingsPath = \"\"\n",
    "text_data = {\"risktext\":[],\"mdatext\":[]}\n",
    "dataStats = {}\n",
    "fieldnames = ['Ticker', \"Company Name\", \"Industry\",\"Top 100\",\"Year\",\"Risk Factors\", \"MDA\"]\n",
    "\n",
    "''' Download latest 10 filings for the Company ''' \n",
    "latest = 10\n",
    "downloadFilings(ticker=ticker, latest = 10)\n",
    "dataStats[ticker] = {\"totalFilings\":0,\"MDAlen\":{},\"Risklen\":{},\"years\":[]}\n",
    "'''  SEC-Edgar module creates a subfolder named by the company ticker inside 'sec_edgar_filings'\n",
    "     Therfore join the path ''' \n",
    "companyFilingsPath = join(newPath, ticker, \"10-K\")\n",
    "\n",
    "''' Sanity Check: if the files are downloaded ''' \n",
    "if isdir(companyFilingsPath):        \n",
    "    ''' List downloded filing path and file names ''' \n",
    "    companyFilings = [(join(companyFilingsPath, f), f) for f in listdir(companyFilingsPath)]\n",
    "    dataStats[ticker][\"totalFilings\"] = len(companyFilings)\n",
    "    ## Log the companies having less than 10 recent filings\n",
    "    if len(companyFilings) <10:\n",
    "        dataStats[ticker] = len(companyFilings)\n",
    "\n",
    "    # Extract Section from each filings\n",
    "    for filing in companyFilings:\n",
    "\n",
    "        # Get filing path and filing path\n",
    "        filingPath, filingName = filing\n",
    "\n",
    "        f = open(filingPath,'r')\n",
    "        textdata = f.read()\n",
    "        f.close()\n",
    "        ''' Get year of the filling from the filing name \n",
    "            file name ex \"0001283630-16-000038.txt\", 2016 represents year in which 10k was filed and it was of 2015 '''\n",
    "        filingName = filingName.split(\"-\")\n",
    "#         print(filingName)\n",
    "        filingYear = int(filingName[1])-1\n",
    "        dataStats[ticker][\"years\"].append(filingYear)\n",
    "        sectionList, data, document = extractHTMLSections(textdata)\n",
    "        risk_text = \"\"\n",
    "        mda_text = \"\"\n",
    "        ## Extracting Risk FActors Section Item 1A\n",
    "        if \"item1a\" in sectionList:\n",
    "            risk_text = extractTextFromSection(\"item1a\", data, document)\n",
    "        else:\n",
    "            print(\"item1a not in section list\", filing)\n",
    "        \n",
    "        text_data[\"risktext\"].append(risk_text)\n",
    "        \n",
    "        if \"item7\" in sectionList:\n",
    "            mda_text = extractTextFromSection(\"item7\", data, document)\n",
    "            print(\"Extracted MDA text Length:{}\".format(len(mda_text)))\n",
    "        else:\n",
    "            print(\"item7 not in section list\", filing)\n",
    "        \n",
    "        text_data[\"mdatext\"].append(mda_text)\n",
    "        \n",
    "        dataStats[ticker][\"MDAlen\"][filingYear] = len(mda_text)\n",
    "        dataStats[ticker][\"Risklen\"][filingYear] = len(risk_text)\n",
    "\n",
    "\n",
    "        \n",
    "else:\n",
    "    print(\"No filings Downloaded for {}\".format(row[\"Company Name\"]))\n",
    "removefilings(join(newPath, ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'totalFilings': 10,\n",
       " 'MDAlen': {16: 60016,\n",
       "  17: 56851,\n",
       "  18: 33501,\n",
       "  9: 65410,\n",
       "  10: 61832,\n",
       "  11: 63361,\n",
       "  12: 65905,\n",
       "  13: 68342,\n",
       "  14: 64159,\n",
       "  15: 58349},\n",
       " 'Risklen': {16: 53078,\n",
       "  17: 55255,\n",
       "  18: 54781,\n",
       "  9: 51523,\n",
       "  10: 51092,\n",
       "  11: 48705,\n",
       "  12: 50965,\n",
       "  13: 53250,\n",
       "  14: 53687,\n",
       "  15: 52302},\n",
       " 'years': [16, 17, 18, 9, 10, 11, 12, 13, 14, 15]}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataStats\n",
    "dataStats[ticker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text_data[\"risktext\"][0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data[\"risktext\"][1][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_lines = text_data[\"risktext\"][4].split(\" \")\n",
    "text2_lines = text_data[\"risktext\"][2].split(\" \")\n",
    "# d = difflib.Differ()\n",
    "# diff = d.compare(text1_lines, text2_lines)\n",
    "# print('\\n'.join(diff))\n",
    "# diff = difflib.ndiff(text1_lines, text2_lines)\n",
    "\n",
    "d = difflib.HtmlDiff()\n",
    "diff = d.make_file(text1_lines, text2_lines)\n",
    "\n",
    "# diff = difflib.unified_diff(text1_lines, text2_lines)\n",
    "# diff = '\\n'.join(list(diff))\n",
    "# diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"compare.html\",\"wb+\")\n",
    "# import unicode\n",
    "f.write(diff.encode('utf8'))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10: {'item1a': 1239, 'item7': 1295}, 14: {'item1a': 1420, 'item7': 1482}, 15: {'item1a': 1474, 'item7': 1522}, 16: {'item1a': 1532, 'item7': 1571}, 17: {'item1a': 1593, 'item7': 1632}, 18: {'item1a': 1666, 'item7': 1708}, 19: {'item1a': 1437, 'item7': 1481}, 11: {'item1a': 1287, 'item7': 1331}, 12: {'item1a': 1320, 'item7': 1359}, 13: {'item1a': 1383, 'item7': 1417}, 9: {'item1a': 229, 'item7': 238}, 3: {'item1a': 0, 'item7': 6}, 0: {'item1a': 0, 'item7': 3}, 1: {'item1a': 0, 'item7': 6}, 2: {'item1a': 0, 'item7': 6}, 4: {'item1a': 1, 'item7': 9}, 5: {'item1a': 9, 'item7': 9}, 7: {'item1a': 7, 'item7': 8}, 8: {'item1a': 6, 'item7': 8}, -1: {'item1a': 0, 'item7': 2}, 98: {'item1a': 0, 'item7': 0}, 6: {'item1a': 7, 'item7': 8}}\n",
      "880\n",
      "1856\n",
      "0.47413793103448276\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 10-K Data Analysis \"\"\"\n",
    "import json\n",
    "# f = open(\"data_10K_1K/dataStats.txt\")\n",
    "f = open(\"data_10K_3K/dataStats.txt\")\n",
    "data = f.read()\n",
    "f.close()\n",
    "data = json.loads(data)\n",
    "ten_filing = 0\n",
    "less_than_ten = 0\n",
    "countdict = {}\n",
    "total = len(data.keys())\n",
    "sections=[\"item1a\", \"item7\"]\n",
    "allrisk = 0\n",
    "allMDA =0\n",
    "for ticker in data.keys():\n",
    "    company = data[ticker]\n",
    "    item7 = False\n",
    "    item1a = False\n",
    "    if company[\"totalfilings\"] == 10:\n",
    "        ten_filing +=1\n",
    "    else:\n",
    "        less_than_ten +=1\n",
    "    if \"item1a\" in company.keys():\n",
    "        item1a = True\n",
    "    if \"item7\" in company.keys():\n",
    "        item7 = True\n",
    "    for i in range(len(company[\"years\"])):\n",
    "        if company[\"years\"][i] not in countdict.keys():\n",
    "            countdict[company[\"years\"][i]] = {\"item1a\":0,\"item7\":0}\n",
    "        if item1a and company[\"item1a\"][i]>100:\n",
    "            countdict[company[\"years\"][i]][\"item1a\"]+=1\n",
    "        if item7 and company[\"item7\"][i]>100:\n",
    "            countdict[company[\"years\"][i]][\"item7\"]+=1\n",
    "print(countdict)\n",
    "coverage = 0\n",
    "total = len(data.keys())\n",
    "for ticker in data.keys():\n",
    "    company = data[ticker]\n",
    "    if \"item1a\" not in company.keys():\n",
    "        continue\n",
    "    if \"item7\" not in company.keys():\n",
    "        continue\n",
    "    item1a = sorted(company[\"item1a\"])\n",
    "    item7 = sorted(company[\"item7\"])\n",
    "    years = sorted(company[\"years\"])\n",
    "    if len(years) == 10 and item1a[0] >100 and item7[0]>100 and years[0] == 10:\n",
    "        coverage += 1\n",
    "print(coverage)\n",
    "print(total)\n",
    "print(coverage/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n",
      "449\n",
      "{9: 303, 10: 318, 11: 319, 12: 334, 13: 336, 14: 334, 15: 341, 16: 348, 17: 354, 18: 368}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 10-Q Data Analysis \"\"\"\n",
    "import json\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "f = open(\"data_10Q_1K/dataStats.txt\")\n",
    "data = f.read()\n",
    "f.close()\n",
    "data = json.loads(data)\n",
    "count = 0\n",
    "item1aCount = 0\n",
    "yearwise = {}\n",
    "for ticker in data.keys():\n",
    "    company = data[ticker]\n",
    "    years = company[\"years\"]\n",
    "    flag = False\n",
    "    sorted_indexes = np.argsort(years)\n",
    "    total = 0\n",
    "    yearTemp = {}\n",
    "    for i in sorted_indexes:\n",
    "        if years[i]>=9 and years[i]<19:\n",
    "            if company[\"item1a\"][i]>100:\n",
    "                if years[i] in yearTemp:\n",
    "                    yearTemp[years[i]]+=1\n",
    "                else:\n",
    "                    yearTemp[years[i]] = 1\n",
    "    for key in yearTemp.keys():\n",
    "        if yearTemp[key] >= 3:\n",
    "            if key in yearwise:\n",
    "                yearwise[key] +=1\n",
    "            else:\n",
    "                yearwise[key] =1\n",
    "    for i in range(9,19):\n",
    "        if i in yearTemp and yearTemp[i] >= 3:\n",
    "            continue\n",
    "        else:\n",
    "            flag = True\n",
    "            break\n",
    "    if not flag:\n",
    "        count +=1\n",
    "print(count)\n",
    "print(len(data.keys()))\n",
    "print(yearwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "f = open(\"data_10K_3K/dataStats.txt\")\n",
    "data_10K = f.read()\n",
    "f.close()\n",
    "data_10K = json.loads(data_10K)\n",
    "\n",
    "f = open(\"data_10Q_1K/dataStats.txt\")\n",
    "data_10Q = f.read()\n",
    "f.close()\n",
    "data_10Q = json.loads(data_10Q)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
