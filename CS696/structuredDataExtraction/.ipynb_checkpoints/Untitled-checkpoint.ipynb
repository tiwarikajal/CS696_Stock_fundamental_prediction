{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "File Name: FilingSummary.xml\n",
      "File Path: https://www.sec.gov/Archives/edgar/data/1265107/000126510719000004/FilingSummary.xml\n"
     ]
    }
   ],
   "source": [
    "# define the base url needed to create the file url.\n",
    "base_url = r\"https://www.sec.gov\"\n",
    "\n",
    "# convert a normal url to a document url\n",
    "normal_url = r\"https://www.sec.gov/Archives/edgar/data/1265107/0001265107-19-000004.txt\"\n",
    "normal_url = normal_url.replace('-','').replace('.txt','/index.json')\n",
    "\n",
    "# define a url that leads to a 10k document landing page\n",
    "documents_url = r\"https://www.sec.gov/Archives/edgar/data/1265107/000126510719000004/index.json\"\n",
    "\n",
    "# request the url and decode it.\n",
    "content = requests.get(documents_url).json()\n",
    "\n",
    "for file in content['directory']['item']:\n",
    "    \n",
    "    # Grab the filing summary and create a new url leading to the file so we can download it.\n",
    "    if file['name'] == 'FilingSummary.xml':\n",
    "\n",
    "        xml_summary = base_url + content['directory']['name'] + \"/\" + file['name']\n",
    "        \n",
    "        print('-' * 100)\n",
    "        print('File Name: ' + file['name'])\n",
    "        print('File Path: ' + xml_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c84d396e43f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# request and parse the content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxml_summary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# find the 'myreports' tag because this contains all the individual reports submitted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m                     \u001b[1;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                     \u001b[1;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                     % \",\".join(features))\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;31m# At this point either we have a TreeBuilder instance in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "# define a new base url that represents the filing folder. This will come in handy when we need to download the reports.\n",
    "base_url = xml_summary.replace('FilingSummary.xml', '')\n",
    "\n",
    "# request and parse the content\n",
    "content = requests.get(xml_summary).content\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "# find the 'myreports' tag because this contains all the individual reports submitted.\n",
    "reports = soup.find('myreports')\n",
    "\n",
    "# I want a list to store all the individual components of the report, so create the master list.\n",
    "master_reports = []\n",
    "\n",
    "# loop through each report in the 'myreports' tag but avoid the last one as this will cause an error.\n",
    "for report in reports.find_all('report')[:-1]:\n",
    "\n",
    "    # let's create a dictionary to store all the different parts we need.\n",
    "    report_dict = {}\n",
    "    report_dict['name_short'] = report.shortname.text\n",
    "    report_dict['name_long'] = report.longname.text\n",
    "    report_dict['position'] = report.position.text\n",
    "    report_dict['category'] = report.menucategory.text\n",
    "    report_dict['url'] = base_url + report.htmlfilename.text\n",
    "\n",
    "    # append the dictionary to the master list.\n",
    "    master_reports.append(report_dict)\n",
    "\n",
    "    # print the info to the user.\n",
    "    print('-'*100)\n",
    "    print(base_url + report.htmlfilename.text)\n",
    "    print(report.longname.text)\n",
    "    print(report.shortname.text)\n",
    "    print(report.menucategory.text)\n",
    "    print(report.position.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_data = []\n",
    "\n",
    "# loop through each statement url\n",
    "for statement in statements_url:\n",
    "\n",
    "    # define a dictionary that will store the different parts of the statement.\n",
    "    statement_data = {}\n",
    "    statement_data['headers'] = []\n",
    "    statement_data['sections'] = []\n",
    "    statement_data['data'] = []\n",
    "    \n",
    "    # request the statement file content\n",
    "    content = requests.get(statement).content\n",
    "    report_soup = BeautifulSoup(content, 'html')\n",
    "\n",
    "    # find all the rows, figure out what type of row it is, parse the elements, and store in the statement file list.\n",
    "    for index, row in enumerate(report_soup.table.find_all('tr')):\n",
    "        \n",
    "        # first let's get all the elements.\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        # if it's a regular row and not a section or a table header\n",
    "        if (len(row.find_all('th')) == 0 and len(row.find_all('strong')) == 0): \n",
    "            reg_row = [ele.text.strip() for ele in cols]\n",
    "            statement_data['data'].append(reg_row)\n",
    "            \n",
    "        # if it's a regular row and a section but not a table header\n",
    "        elif (len(row.find_all('th')) == 0 and len(row.find_all('strong')) != 0):\n",
    "            sec_row = cols[0].text.strip()\n",
    "            statement_data['sections'].append(sec_row)\n",
    "            \n",
    "        # finally if it's not any of those it must be a header\n",
    "        elif (len(row.find_all('th')) != 0):            \n",
    "            hed_row = [ele.text.strip() for ele in row.find_all('th')]\n",
    "            statement_data['headers'].append(hed_row)\n",
    "            \n",
    "        else:            \n",
    "            print('We encountered an error.')\n",
    "\n",
    "    # append it to the master list.\n",
    "    statements_data.append(statement_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grab the proper components\n",
    "income_header =  statements_data[1]['headers'][1]\n",
    "income_data = statements_data[1]['data']\n",
    "\n",
    "# Put the data in a DataFrame\n",
    "income_df = pd.DataFrame(income_data)\n",
    "\n",
    "# Display\n",
    "print('-'*100)\n",
    "print('Before Reindexing')\n",
    "print('-'*100)\n",
    "display(income_df.head())\n",
    "\n",
    "# Define the Index column, rename it, and we need to make sure to drop the old column once we reindex.\n",
    "income_df.index = income_df[0]\n",
    "income_df.index.name = 'Category'\n",
    "income_df = income_df.drop(0, axis = 1)\n",
    "\n",
    "# Display\n",
    "print('-'*100)\n",
    "print('Before Regex')\n",
    "print('-'*100)\n",
    "display(income_df.head())\n",
    "\n",
    "# Get rid of the '$', '(', ')', and convert the '' to NaNs.\n",
    "income_df = income_df.replace('[\\$,)]','', regex=True )\\\n",
    "                     .replace( '[(]','-', regex=True)\\\n",
    "                     .replace( '', 'NaN', regex=True)\n",
    "\n",
    "# Display\n",
    "print('-'*100)\n",
    "print('Before type conversion')\n",
    "print('-'*100)\n",
    "display(income_df.head())\n",
    "\n",
    "# everything is a string, so let's convert all the data to a float.\n",
    "income_df = income_df.astype(float)\n",
    "\n",
    "# Change the column headers\n",
    "income_df.columns = income_header\n",
    "\n",
    "# Display\n",
    "print('-'*100)\n",
    "print('Final Product')\n",
    "print('-'*100)\n",
    "\n",
    "# show the df\n",
    "income_df\n",
    "\n",
    "# drop the data in a CSV file if need be.\n",
    "# income_df.to_csv('income_state.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
